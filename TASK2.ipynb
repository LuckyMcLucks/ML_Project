{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6279f527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATA LOADING\n",
      "\n",
      "DATA VALIDATION\n",
      "\n",
      "Data loaded\n",
      "PREPROCESSING FEATURES\n",
      "Feature dimensions match\n",
      "\n",
      "APPLYING PCA TO COMPONENTS: [2000, 1000, 500, 100]\n",
      "PCA with 2000 components COMPLETE\n",
      "PCA with 1000 components COMPLETE\n",
      "PCA with 500 components COMPLETE\n",
      "PCA with 100 components COMPLETE\n",
      "\n",
      "TRAINING KNN Models (n_neighbors=2)\n",
      "KNN training COMPLETE for 2000 components\n",
      "KNN training COMPLETE for 1000 components\n",
      "KNN training COMPLETE for 500 components\n",
      "KNN training COMPLETE for 100 components\n",
      "\n",
      "PREPARING KAGGLE SUBMISSION FILES\n",
      "Created submission_pca_2000_components.csv\n",
      "Created submission_pca_1000_components.csv\n",
      "Created submission_pca_500_components.csv\n",
      "Created submission_pca_100_components.csv\n",
      "\n",
      "Analysis Complete!\n"
     ]
    }
   ],
   "source": [
    "# Task 2: PCA Dimension Reduction Setup\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "### DATA LOADING SECTION ###\n",
    "print(\"\\nDATA LOADING\")\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "DATA_DIR = \"data/\"\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
    "TEST_CSV = os.path.join(DATA_DIR, \"test.csv\")\n",
    "TRAIN_TFIDF_ZIP = os.path.join(DATA_DIR, \"train_tfidf_features.zip\")\n",
    "TEST_TFIDF_ZIP = os.path.join(DATA_DIR, \"test_tfidf_features.zip\")\n",
    "\n",
    "def extract_zip_to_dataframe(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        \n",
    "        csv_files = [f for f in zip_ref.namelist() if f.endswith('.csv')]\n",
    "        if not csv_files:\n",
    "            raise ValueError(f\"No CSV file found in {zip_path}\")\n",
    "        \n",
    "        csv_filename = csv_files[0]\n",
    "        \n",
    "        with zip_ref.open(csv_filename) as csv_file:\n",
    "            df = pd.read_csv(csv_file)\n",
    "    \n",
    "    return df \n",
    "\n",
    "def load_all_data():\n",
    "    \n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    test_df = pd.read_csv(TEST_CSV)\n",
    "    \n",
    "    # Load TD-IDF features from zip files\n",
    "    X_train = extract_zip_to_dataframe(TRAIN_TFIDF_ZIP)\n",
    "    X_test = extract_zip_to_dataframe(TEST_TFIDF_ZIP)\n",
    "    \n",
    "    # checking if 'label' in columns\n",
    "    if 'label' in train_df.columns:\n",
    "        y_train = train_df['label']\n",
    "    elif 'target' in train_df.columns:\n",
    "        y_train = train_df['target']\n",
    "    else:\n",
    "        return None, None, None, train_df, test_df\n",
    "    \n",
    "    return X_train, y_train, X_test, train_df, test_df\n",
    "\n",
    "\n",
    "### DATA VALIDATION SECTION ###\n",
    "\n",
    "def validate_data(X_train, y_train, X_test):\n",
    "    \n",
    "    print(\"\\nDATA VALIDATION\")\n",
    "    \n",
    "    # testing validation for number of features in zip file and mismatching btw train and test\n",
    "    \"\"\"if X_train.shape[1] == 5000:\n",
    "        print(\"Correct number of features\")\n",
    "    else:\n",
    "        print(f\"Expected 5000 features, got {X_train.shape[1]}\")\n",
    "    \n",
    "    if X_train.shape[1] == X_test.shape[1]:\n",
    "        print(\"Train and test have same number of features\")\n",
    "    else:\n",
    "        print(f\"Feature mismatch - Train: {X_train.shape[1]}, Test: {X_test.shape[1]}\")\"\"\"\n",
    "    \n",
    "    # Check for missing values\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        train_missing = X_train.isnull().sum().sum()\n",
    "        test_missing = X_test.isnull().sum().sum()\n",
    "    else:\n",
    "        train_missing = np.isnan(X_train).sum()\n",
    "        test_missing = np.isnan(X_test).sum()\n",
    "    \n",
    "    if train_missing > 0: \n",
    "        print(f\"Missing values in training: {train_missing}\")\n",
    "    elif test_missing > 0: \n",
    "        print(f\"Missing values in test: {test_missing}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "X_train, y_train, X_test, train_df, test_df = load_all_data()\n",
    "\n",
    "# If successful, validate\n",
    "if X_train is not None:\n",
    "    validate_data(X_train, y_train, X_test)\n",
    "    print(\"\\nData loaded\")\n",
    "else:\n",
    "    print(\"\\nNeed to identify label column in train.csv\")\n",
    "\n",
    "\n",
    "### DATA DEBUGGING FUNCTION - for testing purposes :) ###\n",
    "\n",
    "\"\"\"def debug_data_structure(X_train, X_test):\n",
    "    \n",
    "    print(\"=== DEBUGGING DATA STRUCTURE ===\")\n",
    "    print(f\"X_train type: {type(X_train)}\")\n",
    "    print(f\"X_test type: {type(X_test)}\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    \n",
    "    if hasattr(X_train, 'columns'):\n",
    "        print(f\"X_train columns (first 10): {list(X_train.columns[:10])}\")\n",
    "        print(f\"X_train columns (last 10): {list(X_train.columns[-10:])}\")\n",
    "        print(f\"X_train dtypes: {X_train.dtypes.value_counts()}\")\n",
    "    \n",
    "    if hasattr(X_test, 'columns'):\n",
    "        print(f\"X_test columns (first 10): {list(X_test.columns[:10])}\")\n",
    "        print(f\"X_test columns (last 10): {list(X_test.columns[-10:])}\")\n",
    "        print(f\"X_test dtypes: {X_test.dtypes.value_counts()}\")\n",
    "    \n",
    "    print(\"END DEBUG\\n\")\"\"\"\n",
    "\n",
    "\n",
    "### DATA PREPROCESSING ###\n",
    "\n",
    "def preprocess_features(X_train, X_test):\n",
    "    \n",
    "    print(\"PREPROCESSING FEATURES\")\n",
    "    \n",
    "    # remove 'id' and 'label' columns for training data\n",
    "    train_feature_cols = [col for col in X_train.columns if col not in ['id', 'label']]\n",
    "    X_train_clean = X_train[train_feature_cols]\n",
    "    #print(f\"Removed columns from train: {[col for col in X_train.columns if col not in train_feature_cols]}\")\n",
    "    \n",
    "    # remove 'id' column for test data\n",
    "    test_feature_cols = [col for col in X_test.columns if col not in ['id', 'label']]\n",
    "    X_test_clean = X_test[test_feature_cols]\n",
    "    #print(f\"Removed columns from test: {[col for col in X_test.columns if col not in test_feature_cols]}\")\n",
    "    \n",
    "    common_features = sorted(list(set(train_feature_cols) & set(test_feature_cols)))\n",
    "    \n",
    "    X_train_final = X_train_clean[common_features].values  \n",
    "    X_test_final = X_test_clean[common_features].values    \n",
    "    \n",
    "    if X_train_final.shape[1] == X_test_final.shape[1]:\n",
    "        print(\"Feature dimensions match\")\n",
    "    \n",
    "    return X_train_final, X_test_final\n",
    "\n",
    "\n",
    "### PCA IMPLEMENTATION ###\n",
    "\n",
    "def apply_pca_multiple_components(X_train, X_test, component_list=[2000, 1000, 500, 100]):\n",
    "    \n",
    "    pca_results = {}\n",
    "    \n",
    "    print(f\"\\nAPPLYING PCA TO COMPONENTS: {component_list}\")\n",
    "    \n",
    "    for n_components in component_list:\n",
    "        \n",
    "        # Initialize and fit PCA\n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        pca.fit(X_train)\n",
    "        \n",
    "        X_train_pca = pca.transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        \n",
    "        pca_results[n_components] = {\n",
    "            'pca_model': pca,\n",
    "            'X_train_pca': X_train_pca,\n",
    "            'X_test_pca': X_test_pca,\n",
    "            'explained_variance_ratio': pca.explained_variance_ratio_.sum()\n",
    "        }\n",
    "        \n",
    "        print(f\"PCA with {n_components} components COMPLETE\")\n",
    "        #print(f\"  - Explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "        #print(f\"  - Train PCA shape: {X_train_pca.shape}\")\n",
    "        #print(f\"  - Test PCA shape: {X_test_pca.shape}\")\n",
    "    \n",
    "    return pca_results\n",
    "\n",
    "### KNN TRAINING AND PREDICTION ###\n",
    "\n",
    "def train_knn_and_predict(pca_results, y_train, component_list=[2000, 1000, 500, 100]):\n",
    "\n",
    "    predictions = {}\n",
    "    \n",
    "    print(f\"\\nTRAINING KNN Models (n_neighbors=2)\")\n",
    "    \n",
    "    for n_components in component_list:\n",
    "        if n_components not in pca_results:\n",
    "            continue\n",
    "        \n",
    "        X_train_pca = pca_results[n_components]['X_train_pca']\n",
    "        X_test_pca = pca_results[n_components]['X_test_pca']\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors=2)\n",
    "        \n",
    "        # Train the model\n",
    "        knn.fit(X_train_pca, y_train)\n",
    "        \n",
    "        y_pred = knn.predict(X_test_pca)\n",
    "        \n",
    "        predictions[n_components] = {\n",
    "            'model': knn,\n",
    "            'predictions': y_pred,\n",
    "            'n_predictions': len(y_pred)\n",
    "        }\n",
    "        \n",
    "        print(f\"KNN training COMPLETE for {n_components} components\")\n",
    "        #print(f\"  - Predictions shape: {y_pred.shape}\")\n",
    "        #print(f\"  - Prediction distribution: {np.bincount(y_pred)}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "### KAGGLE SUBMISSION PREPARATION ###\n",
    "\n",
    "def prepare_kaggle_submissions(predictions, test_df, component_list=[2000, 1000, 500, 100]):\n",
    "\n",
    "    submission_files = {}\n",
    "    \n",
    "    print(f\"\\nPREPARING KAGGLE SUBMISSION FILES\")\n",
    "    \n",
    "    for n_components in component_list:\n",
    "        if n_components not in predictions:\n",
    "            continue\n",
    "            \n",
    "        y_pred = predictions[n_components]['predictions']\n",
    "        \n",
    "        # Create submission DataFrame\n",
    "        # Assuming test_df has an 'id' column\n",
    "        if 'id' in test_df.columns:\n",
    "            submission_df = pd.DataFrame({\n",
    "                'id': test_df['id'],\n",
    "                'label': y_pred\n",
    "            })\n",
    "        else:\n",
    "            # If no ID column, create sequential IDs\n",
    "            submission_df = pd.DataFrame({\n",
    "                'id': range(len(y_pred)),\n",
    "                'label': y_pred\n",
    "            })\n",
    "        \n",
    "        filename = f\"submission_pca_{n_components}_components.csv\"\n",
    "        submission_df.to_csv(filename, index=False)\n",
    "        \n",
    "        submission_files[n_components] = filename\n",
    "        \n",
    "        print(f\"Created {filename}\")\n",
    "        #print(f\"  - Shape: {submission_df.shape}\")\n",
    "        #print(f\"  - Sample:\")\n",
    "        #print(submission_df.head())\n",
    "    \n",
    "    return submission_files\n",
    "\n",
    "\n",
    "### MAIN EXECUTION FUNCTION ###\n",
    "\n",
    "def run_complete_pca_analysis(X_train, y_train, X_test, test_df):\n",
    "    \n",
    "    X_train_clean, X_test_clean = preprocess_features(X_train, X_test)\n",
    "    \n",
    "    pca_results = apply_pca_multiple_components(X_train_clean, X_test_clean)\n",
    "    \n",
    "    predictions = train_knn_and_predict(pca_results, y_train)\n",
    "    \n",
    "    submission_files = prepare_kaggle_submissions(predictions, test_df)\n",
    "    \n",
    "    print(f\"\\nAnalysis Complete!\")\n",
    "    \n",
    "    return pca_results, predictions, submission_files\n",
    "\n",
    "\n",
    "# Run the complete PCA analysis\n",
    "pca_results, predictions, submission_files = run_complete_pca_analysis(X_train, y_train, X_test, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9370454",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(100, 2000, 200)  # Try k from 100 to 4000 in steps of 200\n",
    "mean_scores = []\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "for k in k_values:\n",
    "    selector = PCA(X_train, k=k)\n",
    "    X_selected = selector.fit_transform(X_train)\n",
    "    scores = cross_val_score(\n",
    "        LogisticRegression(max_iter=1000, random_state=42), # same baseline model as before\n",
    "        X_selected, y_train, cv=3, scoring='f1'\n",
    "    )\n",
    "    mean_scores.append(scores.mean())\n",
    "\n",
    "plt.plot(k_values, mean_scores, marker='o')\n",
    "plt.xlabel('Number of Features (k)')\n",
    "plt.ylabel('Mean F1 Score (CV)')\n",
    "plt.title('Feature Selection: F1 Score vs k')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "best_k = k_values[np.argmax(mean_scores)]\n",
    "print(\"Best k:\", best_k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
